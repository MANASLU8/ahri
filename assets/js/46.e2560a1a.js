(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{352:function(e,a,t){"use strict";t.r(a);var r=t(10),s=Object(r.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"nlpaper"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#nlpaper"}},[e._v("#")]),e._v(" NLPaper")]),e._v(" "),a("p",[a("strong",[e._v("NLPaper")]),e._v(" is an app that will help you to highlight the most\nimportant information of an ML-related research paper.")]),e._v(" "),a("p",[a("strong",[e._v("Author:")]),e._v(" Aleksei Artemiev.")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/aalksii/nlpaper",target:"_blank",rel:"noopener noreferrer"}},[e._v("Link"),a("OutboundLink")],1),e._v(" to repo.")]),e._v(" "),a("h2",{attrs:{id:"aim"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aim"}},[e._v("#")]),e._v(" Aim")]),e._v(" "),a("p",[e._v("To develop an extractive summarization system for ML-related research papers.")]),e._v(" "),a("h2",{attrs:{id:"tasks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tasks"}},[e._v("#")]),e._v(" Tasks")]),e._v(" "),a("ol",[a("li",[e._v("Research current approaches for extractive summarization;")]),e._v(" "),a("li",[e._v("develop the system architectures (diagrams);")]),e._v(" "),a("li",[e._v("collect and analyze the data from research papers (arXiv);")]),e._v(" "),a("li",[e._v("develop preprocessing pipeline for the text;")]),e._v(" "),a("li",[e._v("pre-train (or prepare) transformer models (DistilBERT and ALBERT) and\ntokenizers;")]),e._v(" "),a("li",[e._v("fine-tune the models using a masked language modeling (MLM) objective;")]),e._v(" "),a("li",[e._v("use the models as feature extractors with a summarizer and get top n\nsentences;")]),e._v(" "),a("li",[e._v("evaluate metrics on the dataset (perplexity) and select the\nbest model based on the metric;")]),e._v(" "),a("li",[e._v("deploy the service on an application server;")]),e._v(" "),a("li",[e._v("optimize the selected model (or data) (i.e. compress it);")])]),e._v(" "),a("h2",{attrs:{id:"dataset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dataset"}},[e._v("#")]),e._v(" Dataset")]),e._v(" "),a("h3",{attrs:{id:"link"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#link"}},[e._v("#")]),e._v(" Link:")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://huggingface.co/datasets/aalksii/ml-arxiv-papers",target:"_blank",rel:"noopener noreferrer"}},[e._v("ðŸ¤— aalksii/ml-arxiv-papers"),a("OutboundLink")],1)]),e._v(" "),a("h3",{attrs:{id:"description"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#description"}},[e._v("#")]),e._v(" Description:")]),e._v(" "),a("p",[e._v("The dataset consists of 117592 research paper abstracts from arXiv.\nThe train and test ratio is 9:1, so it makes 105832 and 11760 rows.\nThe original dataset can be found on\n"),a("a",{attrs:{href:"https://www.kaggle.com/datasets/Cornell-University/arxiv",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaggle"),a("OutboundLink")],1),e._v(" and\nML papers only version on\n"),a("a",{attrs:{href:"https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers",target:"_blank",rel:"noopener noreferrer"}},[e._v("CShorten/ML-ArXiv-Papers"),a("OutboundLink")],1),e._v(".\nThe average length of the abstracts is 1157 symbols.")]),e._v(" "),a("h3",{attrs:{id:"expediency-of-its-use"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#expediency-of-its-use"}},[e._v("#")]),e._v(" Expediency of its use:")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("The abstracts can be used to fine-tune BERT-based models using masked language\nmodeling technique. Since a BERT model was pre-trained using only an unlabeled,\nplain text corpus (English Wikipedia, the Brown Corpus), it can be less\nprepared for a scientific language such as that found in arXiv dataset.\nHowever, the dataset can be edited with masking and fed into the models. Then\nit is possible to use such a fine-tuned model for sentence embeddings.")])]),e._v(" "),a("li",[a("p",[e._v("The topic of all papers in the dataset is machine learning, so it should\nbe easier for a model to adapt to a new domain.")])]),e._v(" "),a("li",[a("p",[e._v("The selected models are much more compact compared to BERT.\nTherefore, it is possible to train these models using a single GPU machines,\nsuch as Google Colab.")])])]),e._v(" "),a("h2",{attrs:{id:"project-diagrams"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#project-diagrams"}},[e._v("#")]),e._v(" Project diagrams")]),e._v(" "),a("h3",{attrs:{id:"component-diagram"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#component-diagram"}},[e._v("#")]),e._v(" Component diagram")]),e._v(" "),a("p",[a("img",{attrs:{src:"media/component_diagram.jpg",alt:"component_diagram"}}),e._v(" "),a("em",[e._v("Figure 1. Pipeline components")])]),e._v(" "),a("h3",{attrs:{id:"communication-diagram"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#communication-diagram"}},[e._v("#")]),e._v(" Communication diagram")]),e._v(" "),a("p",[a("img",{attrs:{src:"media/communication_diagram.jpg",alt:"communication_diagram"}}),e._v(" "),a("em",[e._v("Figure 2. Text processing communication pipeline")])]),e._v(" "),a("h3",{attrs:{id:"activity-diagram"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#activity-diagram"}},[e._v("#")]),e._v(" Activity diagram")]),e._v(" "),a("p",[a("img",{attrs:{src:"media/activity_diagram.jpg",alt:"activity_diagram"}}),e._v(" "),a("em",[e._v("Figure 3. Model usage pipeline")])]),e._v(" "),a("h3",{attrs:{id:"deployment-diagram"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deployment-diagram"}},[e._v("#")]),e._v(" Deployment diagram")]),e._v(" "),a("p",[a("img",{attrs:{src:"media/deployment_diagram.jpg",alt:"deployment_diagram"}}),e._v(" "),a("em",[e._v("Figure 4. Deployment pipeline")])]),e._v(" "),a("h2",{attrs:{id:"data-preparation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#data-preparation"}},[e._v("#")]),e._v(" Data preparation")]),e._v(" "),a("p",[e._v("The "),a("a",{attrs:{href:"https://huggingface.co/datasets/aalksii/ml-arxiv-papers",target:"_blank",rel:"noopener noreferrer"}},[e._v("manually"),a("OutboundLink")],1),e._v("\ncreated dataset (look at the notebook to check how it was done) is loaded on\nðŸ¤— public repository. In this project, I use ðŸ¤— API to load data from this repo.\nAll the parameters can be changed using configuration files in "),a("code",[e._v("src")]),e._v(" directory.")]),e._v(" "),a("h2",{attrs:{id:"pre-training-and-fine-tuning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pre-training-and-fine-tuning"}},[e._v("#")]),e._v(" Pre-training and fine-tuning")]),e._v(" "),a("p",[e._v("The pipeline for training used in the project is:")]),e._v(" "),a("ol",[a("li",[e._v("Load model weights from "),a("a",{attrs:{href:"https://huggingface.co/aalksii/distilbert-base-uncased-ml-arxiv-papers",target:"_blank",rel:"noopener noreferrer"}},[e._v("aalksii/distilbert-base-uncased-ml-arxiv-papers"),a("OutboundLink")],1),e._v(" and\n"),a("a",{attrs:{href:"https://huggingface.co/aalksii/albert-base-v2-ml-arxiv-papers",target:"_blank",rel:"noopener noreferrer"}},[e._v("aalksii/albert-base-v2-ml-arxiv-papers"),a("OutboundLink")],1),e._v("\n-- these models are DistilBERT and ALBERT pre-trained models which fine-tuned\non the part of the dataset, but we could skip this step and use common versions\nof the models.")]),e._v(" "),a("li",[e._v("Pre-train: use part of the dataset to train these models.")]),e._v(" "),a("li",[e._v("Fine-tune: the same as 2, however, we use pre-trained models from step 2.")])]),e._v(" "),a("h2",{attrs:{id:"choosing-the-optimal-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#choosing-the-optimal-model"}},[e._v("#")]),e._v(" Choosing the optimal model")]),e._v(" "),a("p",[e._v("To choose the best model among fine-tuned, I compare them using few metrics.\nThe formula to get the score is:\n"),a("code",[e._v("score(model)=RelativeChange(Perplexity(model))+RelativeChange(InferenceTime(model))+1/InferenceTime(model)")]),e._v(",\nwhere "),a("code",[e._v("RelativeChange")]),e._v(" is computed for pre-trained and fine-tuned models.\nAfter we compute score for each model, we can use "),a("code",[e._v("argmax")]),e._v(" to select\nthe best one.")]),e._v(" "),a("h2",{attrs:{id:"deployment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deployment"}},[e._v("#")]),e._v(" Deployment")]),e._v(" "),a("p",[e._v("The first way to deploy the service was Heroku, but it was hard to create\na container with the size less than 500 MB (my Python's cache on\nGitHub takes 2 GB). So I decided to move to DigitalOcean (thanks to GitHub's\neducation pack) and created a droplet with 2 GB RAM, 1 vCPU, and 50 GB SSD.\nAfter this, I launched a GitHub Actions service as a self-hosted machine to use\nwith the repo (take a look at the workflow file). To process a text and\nget a summary, we send request to "),a("code",[e._v("localhost")]),e._v(", which is hosted by server\nwith REST API. The server uses fine-tuned models to predict the result.")]),e._v(" "),a("h2",{attrs:{id:"next-steps"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#next-steps"}},[e._v("#")]),e._v(" Next steps")]),e._v(" "),a("p",[e._v("Next possible steps to take:")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("develop a cross-validation evaluation pipeline to\nensure that perplexity is not affected by random masking;")])]),e._v(" "),a("li",[a("p",[e._v("replace latex symbols and urls with a new token\nto let the model pay attention on it;")])]),e._v(" "),a("li",[a("p",[e._v("use other BERT-based architectures;")])])])])}),[],!1,null,null,null);a.default=s.exports}}]);